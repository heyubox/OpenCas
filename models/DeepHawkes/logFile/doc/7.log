===========DeepHawkes loading cascade===========
save valid id in:../data/deephawkes/dblp/
total_readin: 151247
discard_midnight: 0
discard_outer: 60707
total: 90540
train data: 63377
valid data: 13582
test data: 13581
train test valid 63377 13581 13582
total data: 90540
total time in preprocessing: 36.202579736709595
63377 13582 13581
max_size 100
max_size 100
max_size 100
Number of sequence: 100
120.0
max_num: 100
max_num: 100
max_num: 100

 length of sequence: 16
14 16 13
length of original isd: 453895
length of original isd: 195211
length of original isd: 194301
lenth of original_ids: 516575
63377
blank_template 16 [516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573]
data num: 63377 17 16
num remove:0 in dataset
blank_template 16 [516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573]
data num: 13582 15 16
num remove:0 in dataset
blank_template 16 [516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573, 516573]
data num: 13581 23 16
num remove:0 in dataset
(516575, 120.0, 16)
31.722253561019897
os.environ: 0
1.14.0
ok
dataset information: nodes:516575, n_sequence:120, n_steps:16 
===================configuration===================
dropout prob :  0.001
l2 0.05
learning rate :  0.005
emb_learning_rate :  0.005
observation hour [7,19]
observation threshold :  7
prediction time :  86400
cascade length [10,100]
model save at : ../model_save/deephawkes/dblp/
===================configuration===================
---------***---------
Number: 63377, Max: 12.06339508128851 and Min: 0.0 label Value in Train
NUmber: 13581, Max: 10.51175265376738 and Min: 0.0 label Value in Test
NUmber: 13582, Max: 11.385323176175872 and Min: 0.0 label Value in Valid
---------***---------
63377 13581 13582
Total_parameters:25854006
finish one epoch, time317.1232907772064

train mseloss : 1.5214474996800433
lambda1 [ 0.01 -0.02 -0.    0.01  0.04  0.07  0.12]
#0, Training Loss= 1.52235, Valid Loss= 1.52431, Valid median Loss= 0.66762, Best Valid Loss= 1.52431, Test Loss= 1.51328, Test median  Loss= 0.67201, Best Test Loss= 1.51328
finish one epoch, time1292.7658047676086

train mseloss : 1.488952784487858
lambda1 [ 0.01 -0.02 -0.    0.01  0.04  0.08  0.13]
#1, Training Loss= 1.48745, Valid Loss= 1.47569, Valid median Loss= 0.63210, Best Valid Loss= 1.47569, Test Loss= 1.48361, Test median  Loss= 0.64265, Best Test Loss= 1.48361
finish one epoch, time2603.7370641231537

train mseloss : 1.4649292599847028
lambda1 [ 0.01 -0.01  0.    0.02  0.04  0.08  0.14]
#2, Training Loss= 1.46471, Valid Loss= 1.46134, Valid median Loss= 0.62777, Best Valid Loss= 1.46134, Test Loss= 1.45650, Test median  Loss= 0.63719, Best Test Loss= 1.45650
finish one epoch, time3999.4673702716827

train mseloss : 1.4784406706915103
lambda1 [-0.   -0.02  0.    0.02  0.04  0.09  0.15]
#3, Training Loss= 1.47637, Valid Loss= 1.47492, Valid median Loss= 0.64546, Best Valid Loss= 1.46134, Test Loss= 1.47129, Test median  Loss= 0.64202, Best Test Loss= 1.45650
finish one epoch, time5069.49778342247

train mseloss : 1.4636185140159534
lambda1 [ 0.   -0.01  0.01  0.01  0.04  0.09  0.14]
#4, Training Loss= 1.46339, Valid Loss= 1.46157, Valid median Loss= 0.63451, Best Valid Loss= 1.46134, Test Loss= 1.46209, Test median  Loss= 0.63855, Best Test Loss= 1.45650
finish one epoch, time6067.315032482147

train mseloss : 1.4547343383511828
lambda1 [ 0.01 -0.02  0.    0.02  0.05  0.1   0.14]
#5, Training Loss= 1.45636, Valid Loss= 1.44825, Valid median Loss= 0.62802, Best Valid Loss= 1.44825, Test Loss= 1.45230, Test median  Loss= 0.62889, Best Test Loss= 1.45230
finish one epoch, time7069.705936670303

train mseloss : 1.4593543180038737
lambda1 [ 0.02 -0.02 -0.    0.02  0.05  0.09  0.15]
#6, Training Loss= 1.45816, Valid Loss= 1.45493, Valid median Loss= 0.62083, Best Valid Loss= 1.44825, Test Loss= 1.45213, Test median  Loss= 0.62520, Best Test Loss= 1.45213
finish one epoch, time8072.799019813538

train mseloss : 1.456643360507747
lambda1 [ 0.01 -0.01 -0.    0.02  0.06  0.09  0.15]
#7, Training Loss= 1.45566, Valid Loss= 1.45014, Valid median Loss= 0.62846, Best Valid Loss= 1.44825, Test Loss= 1.45056, Test median  Loss= 0.62315, Best Test Loss= 1.45056
finish one epoch, time9074.95478630066

train mseloss : 1.4716320358159987
lambda1 [ 0.01 -0.02 -0.    0.02  0.05  0.1   0.16]
#8, Training Loss= 1.47232, Valid Loss= 1.46343, Valid median Loss= 0.63734, Best Valid Loss= 1.44825, Test Loss= 1.46692, Test median  Loss= 0.65027, Best Test Loss= 1.45056
finish one epoch, time10061.625144481659

train mseloss : 1.461240767183316
lambda1 [-1.98e-05 -2.33e-02 -1.62e-04  1.90e-02  5.51e-02  9.94e-02  1.64e-01]
#9, Training Loss= 1.45913, Valid Loss= 1.45808, Valid median Loss= 0.63394, Best Valid Loss= 1.44825, Test Loss= 1.45333, Test median  Loss= 0.63357, Best Test Loss= 1.45056
finish one epoch, time11034.086577177048

train mseloss : 1.451840488601436
lambda1 [ 0.   -0.02 -0.01  0.03  0.06  0.1   0.16]
#10, Training Loss= 1.45213, Valid Loss= 1.44589, Valid median Loss= 0.61983, Best Valid Loss= 1.44589, Test Loss= 1.44352, Test median  Loss= 0.63090, Best Test Loss= 1.44352
finish one epoch, time12019.8793258667

train mseloss : 1.4795298815217874
lambda1 [ 0.01 -0.02 -0.01  0.03  0.06  0.1   0.14]
#11, Training Loss= 1.48098, Valid Loss= 1.48211, Valid median Loss= 0.64378, Best Valid Loss= 1.44589, Test Loss= 1.47630, Test median  Loss= 0.65283, Best Test Loss= 1.44352
finish one epoch, time12998.332059144974

train mseloss : 1.4554169667676997
lambda1 [ 0.01 -0.01  0.    0.03  0.06  0.11  0.16]
#12, Training Loss= 1.45356, Valid Loss= 1.45051, Valid median Loss= 0.61936, Best Valid Loss= 1.44589, Test Loss= 1.44236, Test median  Loss= 0.62342, Best Test Loss= 1.44236
finish one epoch, time13980.377260923386

train mseloss : 1.4679215787970126
lambda1 [-0.   -0.02 -0.01  0.02  0.05  0.1   0.17]
#13, Training Loss= 1.46842, Valid Loss= 1.46892, Valid median Loss= 0.63695, Best Valid Loss= 1.44589, Test Loss= 1.46755, Test median  Loss= 0.63838, Best Test Loss= 1.44236
finish one epoch, time14970.007880926132

train mseloss : 1.4573053423091604
lambda1 [ 0.01 -0.02  0.    0.02  0.06  0.1   0.17]
#14, Training Loss= 1.45792, Valid Loss= 1.45403, Valid median Loss= 0.62420, Best Valid Loss= 1.44589, Test Loss= 1.44929, Test median  Loss= 0.62712, Best Test Loss= 1.44236
finish one epoch, time15948.935220479965

train mseloss : 1.4929992075192715
lambda1 [ 0.02 -0.01  0.    0.02  0.05  0.11  0.18]
#15, Training Loss= 1.49361, Valid Loss= 1.48135, Valid median Loss= 0.63641, Best Valid Loss= 1.44589, Test Loss= 1.48305, Test median  Loss= 0.65100, Best Test Loss= 1.44236
finish one epoch, time16958.217185258865

train mseloss : 1.466053198085778
lambda1 [ 0.01 -0.02 -0.    0.02  0.06  0.12  0.17]
#16, Training Loss= 1.46738, Valid Loss= 1.45343, Valid median Loss= 0.62859, Best Valid Loss= 1.44589, Test Loss= 1.45833, Test median  Loss= 0.63416, Best Test Loss= 1.44236
finish one epoch, time17940.83408999443

train mseloss : 1.458493712171704
lambda1 [ 0.01 -0.01 -0.    0.03  0.06  0.1   0.18]
#17, Training Loss= 1.45860, Valid Loss= 1.45232, Valid median Loss= 0.62050, Best Valid Loss= 1.44589, Test Loss= 1.45139, Test median  Loss= 0.63512, Best Test Loss= 1.44236
Finished!
----------------------------------------------------------------
Time: 18483.68410038948
Valid Loss: 1.4458904
Test Loss: 1.4423597
