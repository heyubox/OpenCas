{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def invisible(ax):\n",
    "    # ax=plt.gca()  #gca:get current axis得到当前轴\n",
    "    #设置图片的右边框和上边框为不显示\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['left'].set_color('none')\n",
    "    ax.spines['bottom'].set_color('none')\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    # x 轴不可见\n",
    "    ax.axes.get_xaxis().set_visible(False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dir = 'citation1'\n",
    "# dir = 'weibo1'\n",
    "valid = pickle.load(open(\"/data/cas2vec/\"+dir+\"/valid_ana.pkl\",'rb'))\n",
    "test = pickle.load(open(\"/data/cas2vec/\"+dir+\"/test_ana.pkl\",'rb'))\n",
    "valid.keys(),test.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(dict_keys(['outputs_prediction', 'outputs_true', 'graph_hidden', 'rnn_hidden', 'graph_info', 'ptr', 'nodes_f', 'nodes_map']),\n",
       " dict_keys(['outputs_prediction', 'outputs_true', 'graph_hidden', 'rnn_hidden', 'graph_info', 'ptr', 'nodes_f', 'nodes_map']))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "nodes_f = test['nodes_f']+valid['nodes_f']\n",
    "nodes_map = test['nodes_map']+valid['nodes_map']\n",
    "ptr = np.concatenate((test['ptr'],valid['ptr']))\n",
    "test_trues = np.concatenate((test['outputs_true'],valid['outputs_true']))\n",
    "test_graph_size = []\n",
    "test_graph_hidden = np.concatenate((test['graph_hidden'],valid['graph_hidden']))\n",
    "test_rnn_hidden = np.concatenate((test['rnn_hidden'],valid['rnn_hidden']))\n",
    "graph_info = test['graph_info']+valid['graph_info']\n",
    "n_seq = test['graph_hidden'].shape[2]\n",
    "hidden_dim = test['graph_hidden'].shape[-1]\n",
    "\n",
    "for iter in ptr:\n",
    "    for idx in range(0,len(iter)-1,n_seq):\n",
    "        one_batch = iter[idx+1:idx+n_seq+1]-iter[idx:idx+n_seq]\n",
    "        test_graph_size.append(one_batch)\n",
    "# test_graph_size=np.array(test_graph_size)\n",
    "test_graph_size=np.log2(np.array(test_graph_size)+1)\n",
    "# print(test_graph_size.shape)\n",
    "last_moment = test_graph_size.T[-1]\n",
    "# print(last_moment)\n",
    "# print(np.expand_dims(test_graph_size.T[-1],axis=1)-test_graph_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "\n",
    "def get_hour(time_str, filename):\n",
    "    hour = None\n",
    "    try:\n",
    "        msg_time = int(time_str)\n",
    "        hour = time.strftime(\"%H\", time.localtime(msg_time))\n",
    "        hour = int(hour)\n",
    "    except:\n",
    "        if '170w' in filename:  # fixed in 11.15, however, in this way, more datasets will be removed\n",
    "            ts = time.strptime(time_str, '%Y-%m-%d-%H:%M:%S')\n",
    "            hour = ts.tm_hour\n",
    "        elif 'castle' in filename:\n",
    "            # for data castle weibo\n",
    "            hour = int(time_str[:2])\n",
    "        elif 'smp' in filename:\n",
    "            ts = time.strptime(time_str, '%Y-%m-%dT%H:%M:%S')\n",
    "            hour = ts.tm_hour\n",
    "        else:\n",
    "            print(time_str)\n",
    "            print('wrong time format')\n",
    "    return hour\n",
    "import networkx as nx\n",
    "def construct_G(filename='/home/ta/yh/open-cas/rawdata/dataset_weibo.txt'):\n",
    "    G = nx.Graph()\n",
    "    cascade_count={}\n",
    "    # filename='./dataset_citation.txt'\n",
    "    discard_midnight = 0\n",
    "    discard_outer = 0       \n",
    "    file = open(filename)\n",
    "    cascades_total = dict()\n",
    "    year_grow = dict()\n",
    "    num_lines = sum(1 for line in open(filename, 'r'))\n",
    "    for line in tqdm(file, total=num_lines):\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 5:\n",
    "            print('wrong format!')\n",
    "            continue\n",
    "        cascadeID = parts[0]\n",
    "        # print cascadeID\n",
    "        n_nodes = int(parts[3])\n",
    "\n",
    "        path = parts[4].split(\" \")\n",
    "        if n_nodes != len(path):\n",
    "            print(cascadeID,' wrong number of nodes', n_nodes, len(path),path)\n",
    "            exit(0)\n",
    "        # for p in path:\n",
    "        #     nodes = p.split(\":\")[0].split(\"/\")\n",
    "        #     time_ = int(int(p.split(\":\")[1]))\n",
    "        #     if time_<=3600:\n",
    "        #         num+=1\n",
    "        # if num>100 :\n",
    "        #     discard_outer+=1\n",
    "        #     continue\n",
    "        # if n_nodes<5 or n_nodes>100:\n",
    "        #     continue\n",
    "        hour = get_hour(parts[2], filename)\n",
    "        # to keep the same with\n",
    "        if hour <= 7 or hour >= 19:  # 8-18\n",
    "            discard_midnight += 1\n",
    "            continue\n",
    "        observation_path = []\n",
    "        # print(cascadeID)\n",
    "        for p in path:\n",
    "            nodes = p.split(\":\")[0].split(\"/\")\n",
    "            time_ = int(int(p.split(\":\")[1])/360)\n",
    "            try:\n",
    "                year_grow[time_]+=1\n",
    "            except:\n",
    "                year_grow[time_]=1\n",
    "            nodes_ok = True\n",
    "            for n in nodes:\n",
    "                if int(n) == -1:  # delete invalid id\n",
    "                    nodes_ok = False\n",
    "            if not (nodes_ok):\n",
    "                print(\"error id at cas_id {}\".format(cascadeID))\n",
    "                print(nodes)\n",
    "                continue\n",
    "            # print nodes\n",
    "            time_now = int(p.split(\":\")[1])\n",
    "\n",
    "            observation_path.append(\",\".join(nodes) + \":\" + str(time_now))\n",
    "            for i in range(1, len(nodes)):\n",
    "                G.add_edge(nodes[i - 1] , nodes[i])\n",
    "        # if len(observation_path)<5 or  len(observation_path) > 1000:\n",
    "        #     # least_rewteetnum = opts.least_num   # 5 or 10\n",
    "        #     # if len(observation_path) < least_rewteetnum:\n",
    "            \n",
    "        #     discard_outer += 1\n",
    "        #     continue\n",
    "\n",
    "\n",
    "        # if labels[0]>1000:\n",
    "        #     continue\n",
    "\n",
    "\n",
    "        # try:\n",
    "        #     cascades_total[cascadeID] = msg_time\n",
    "        # except:\n",
    "        cascades_total[cascadeID] = hour\n",
    "\n",
    "    n_total = len(cascades_total)\n",
    "    print(\"total_readin:\", num_lines)\n",
    "    print(\"discard_midnight:\", discard_midnight)\n",
    "    print(\"discard_outer:\", discard_outer)\n",
    "    print('total:', n_total)\n",
    "    # import operator\n",
    "    # sorted_msg_time = sorted(cascades_total.items(),\n",
    "    #                          key=operator.itemgetter(1))\n",
    "    # 计算每个节点的 PR 值，并作为节点的 pagerank 属性\n",
    "    print('computing pange rank')\n",
    "    pagerank = nx.pagerank(G)\n",
    "    # 将 pagerank 数值作为节点的属性\n",
    "    nx.set_node_attributes(G, name = 'pagerank', values=pagerank)\n",
    "    print('computing done')\n",
    "\n",
    "    return G\n",
    "    # 划分数据集\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "G = construct_G('/home/ta/yh/open-cas/rawdata/dataset_citation.txt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 72936/72936 [00:18<00:00, 4006.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total_readin: 72936\n",
      "discard_midnight: 0\n",
      "discard_outer: 0\n",
      "total: 72936\n",
      "computing pange rank\n",
      "computing done\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "list(G.nodes())[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['855', '861', '873', '866', '856', '868', '864', '862', '877', '865']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "page_rank = nx.get_node_attributes(G, \"pagerank\")\n",
    "# page_rank['1130594']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# nodes_f = test['nodes_f']+valid['nodes_f']\n",
    "# nodes_map = test['nodes_map']+valid['nodes_map']\n",
    "node2f = {}\n",
    "for batch_f,batch_map in zip(nodes_f,nodes_map):\n",
    "    for index,node in  batch_map.items():\n",
    "        try:\n",
    "            node2f[node].append(batch_f[index])\n",
    "        except:\n",
    "            node2f[node]=[batch_f[index]]\n",
    "# id2index={}\n",
    "# for "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(node2f),len(G)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(279862, 4818928)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "X = []\n",
    "y = []\n",
    "import random\n",
    "for k,v in node2f.items():\n",
    "    y.append(page_rank[k])\n",
    "    X.append(np.array(node2f[k][-1]))\n",
    "    # X.append(np.array(random.choice(node2f[k])))\n",
    "y=np.array(y)\n",
    "X=np.array(X)\n",
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(127504, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "avg = np.average(y)\n",
    "top = sorted(y,reverse=True)[int(len(y)*0.01)]\n",
    "sample = int(len(y)*0.01)\n",
    "print(sample)\n",
    "print(avg)\n",
    "label = []\n",
    "new_X = []\n",
    "for i,x in zip(y,X):\n",
    "    if i>top:\n",
    "        label.append(1)\n",
    "        new_X.append(x)\n",
    "    # elif i > avg*0.9:\n",
    "    #     label.append(1)\n",
    "    elif sample > 0:\n",
    "        sample-=1\n",
    "        label.append(-1)\n",
    "        new_X.append(x)\n",
    "# X label\n",
    "label=np.array(label)\n",
    "np.sum(label==0),np.sum(label==1),np.sum(label==-1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1275\n",
      "4.298437295985787e-06\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, 1275, 1275)"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_X, label,test_size=0.25, stratify = label)\n",
    "clf = MLPClassifier(random_state=1).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test)\n",
    "clf.predict(X_test)\n",
    "mlp_predictions = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, mlp_predictions)\n",
    "clf.score(X_test, y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ta/.local/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5815047021943573"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print(cm)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[215 104]\n",
      " [201 118]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(graph_info),len(test_graph_hidden),len(graph_info[0]),len(test_graph_hidden[0]),test_graph_hidden.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(nodes_f),len(nodes_f[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(np.max(graph_info[2][0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test['graph_hidden'].shape,valid['graph_hidden'].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_pool = []\n",
    "for outputs in test_graph_hidden:\n",
    "    outputs = torch.tensor(outputs)\n",
    "    lstm_output = F.adaptive_max_pool1d(outputs.transpose(1, 2), output_size=1).squeeze()\n",
    "    output_pool.append(lstm_output.tolist())\n",
    "output_pool = np.array(output_pool)\n",
    "output_pool.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## graph_hidden-graph_size"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.utils import shuffle\n",
    "print(test_graph_size.shape,test_graph_hidden.shape)\n",
    "plt.close()\n",
    "invisible(plt.gca())\n",
    "Hs,Is = shuffle(test_graph_hidden.reshape(-1,hidden_dim),np.power(2,test_graph_size).reshape(-1))\n",
    "X = TSNE(n_components=2,random_state=42,learning_rate=25,n_iter=6000,verbose=1).fit_transform(Hs[:80000])\n",
    "sc = plt.scatter(X[:, 0], X[:, 1],s=1, c=Is[:80000], cmap='viridis_r')\n",
    "plt.colorbar(sc)\n",
    "plt.savefig(fname='graph_hidden-graph_size.png',dpi=600)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## max_pooling-future_growth"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_trues.shape,output_pool.reshape(-1,64).shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.close()\n",
    "Hs,Is = [],[]\n",
    "for o,t,l in zip(output_pool.reshape(-1,64),test_trues,last_moment):\n",
    "    if l < 3:\n",
    "        continue\n",
    "    # if t>8:\n",
    "    #     continue\n",
    "    # if t<4:\n",
    "    #     continue\n",
    "    Hs.append(o)\n",
    "    Is.append(t)\n",
    "Hs=np.array(Hs)\n",
    "Hs,Is = shuffle(Hs,Is)\n",
    "print(Hs.shape)\n",
    "print(np.max(Is))\n",
    "plt.close()\n",
    "invisible(plt.gca())\n",
    "X = TSNE(n_components=2,random_state=42,learning_rate=10,n_iter=6000,verbose=1).fit_transform(Hs[:10000])\n",
    "sc = plt.scatter(X[:, 0], X[:, 1],s=1, c=Is[:10000], cmap='viridis_r')\n",
    "plt.colorbar(sc)\n",
    "plt.savefig(fname='max_pooling-future_growth.png',dpi=600)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inc = []\n",
    "for b in test_graph_size:\n",
    "    inc.append(np.power(2,b[1:])-np.power(2,b[:-1]))\n",
    "diff_hidden = []\n",
    "for h in test_graph_hidden.reshape(-1,n_seq,hidden_dim):\n",
    "    diff_hidden.append(h[1:]-h[:-1])\n",
    "inc = np.array(inc)\n",
    "diff_hidden = np.array(diff_hidden)\n",
    "print(inc.shape,diff_hidden.shape)\n",
    "print(inc.min(),inc.max())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dHs = []\n",
    "dIs = []\n",
    "for h,i in zip(diff_hidden.reshape(-1,hidden_dim),inc.reshape(-1)):\n",
    "    if i<2:\n",
    "        continue\n",
    "    dHs.append(h)\n",
    "    dIs.append(np.log2(i))\n",
    "\n",
    "print(len(dHs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## difference_hidden-diff_size"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.close()\n",
    "dHs,dIs = shuffle(dHs,dIs)\n",
    "plt.close()\n",
    "invisible(plt.gca())\n",
    "X = TSNE(n_components=2,random_state=42,learning_rate=25,n_iter=3000,verbose=1).fit_transform(dHs[:10000])\n",
    "sc = plt.scatter(X[:, 0], X[:, 1],s=1, c=dIs[:10000], cmap='viridis_r')\n",
    "plt.colorbar(sc)\n",
    "# 效果不错，数据有些少\n",
    "plt.savefig(fname='difference-diff_size.png',dpi=600)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# d_hidden, diff_inc=diff_hidden.reshape(-1,hidden_dim),inc.reshape(-1)\n",
    "# model_mlp = MLPRegressor(\n",
    "#     hidden_layer_sizes=(hidden_dim,1),  activation='relu', solver='adam', alpha=0.0001, batch_size='auto',\n",
    "#     learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=50, shuffle=True,\n",
    "#     random_state=1, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "#     early_stopping=False,beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "# model_mlp.fit(d_hidden, diff_inc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mlp_pre=model_mlp.predict(d_hidden)\n",
    "# print(mlp_pre,diff_inc)\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import r2_score\n",
    "# from sklearn.metrics import explained_variance_score\n",
    "# mean_squared_error(mlp_pre, diff_inc),r2_score(mlp_pre, diff_inc),explained_variance_score(mlp_pre, diff_inc)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50a9721e009ea04b57a16cc66bbe10ba764922a3928e20a70e7bd5160a15a3be"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.2 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}